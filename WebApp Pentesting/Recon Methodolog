Recon Methodology:
=================
Why do we do Recon:(Read this:: Important)
	1.Leaked credentials
	2.Misconfigured systems
	3.Outdated patch
	Recon is NOT about exploring the scope, its about DOING the above mentioned points.
AIM: To spend months on a particular program
Step-1 Choosing a program based on parameters:
	Program launch date:
		more age => more dupes
	Program responsiveness:
		average time to resolve/fix a security issue
		more time => more chance of dupes
	The scope of the bug bounty program:
		bigger scopes: 
			low competition
			big companies have different teams, it means differnt jobs which means more number of mistakes being made
	Bug bounty rewards
Step-2 Once a program is chosen:
	Finding already diclosed bugs in that program:
		Read hacktivity for that program
		google: site.com XSS vulnerability
	Doing the Recon:
		A. Finding Domains:(root/TLC/seeds)
			copy them from BB page
			if a bb program has line like "Everything owned by company is allowed":
				to find all of its assets
				Acquisitions:
					some sites are owned by company but' are not presnt on BB page, these sites are mostly not in use, but still online
					crunchbase.com
						lets say company A owns company X,Y,Z and has all scope BB program. company Z owns comapany P,Q,R. so, if i find a bug in company P, then, its a valid finding to report to company A
				ASN enumeration:(autonomus system number)
					AS<number>
					manual way:
						bgp.he.net
							search for "company name"
							issue:
								doesnt represent cloud spaces owned by company
					Automated tools:
						metabigor:(github)
							uses bgp.he.net and asnlookup.com
							echo "comp_name" | metabigor --org -v
						asnlookup:(github)
							python asnlookup.py -o <comp_name>
						Amass:(best)
							amass intel -asn <number>
							it shows domain/ip attached with ASN
				Reverse Whois:
					it tells us all domain registered to company.com
					Note:
						some domains are made to prevent phishing or for market campaign
					manual way:
						whoxy.com
					Automated way:
						amass:
							amass intel -whois -d domain.com -o out.txt
						Domlink:
							domlink.py -d site.com -o site.txt
				Ad Analytics and relationships:
					site.com and all its owned domains have same google analytics code and this way, we can find other domains attached with a site
					manual way:
						builtwith.com/relationships/
							search "site.com"
							goto "tag historry and relationships" => other domains
					automated way:
						getrelationship.py:(github by M4LL0k)
							echo "site.com" | python3 getrelationship.py
		B. Subdomain Enumeration:
			Linked discovery:
				finding links of our target using a SPIDER such as burp
				Manual way:
					BURP PRO:
						configuration:
							target -> scope -> enable "advanced Scope Control"
							target-> scope -> include in scope -> Add -> host: site_name -> ok
								site_name is our keyword here
							target -> sitemap -> filter -> enable "show only in-scope items"
						spidering:
							select all targets in sitemap -> right click -> spider these targets
								Note:
									do this recursively, until there is nothing left to find
							this will show us newly found subdomains and even new seed domains and it will higlight them
						to get the data out:
							target -> sitemap -> select all targets -> right-click -> engagement tools -> analyze target -> save report as file.html file
							open file.html -> go to target section => copy all targets from there
				Automated way:
					Gospider:(github)
						installation:
							apt-get install gospider
						usage:
							it is a SPIDER, it means it will find everything which return a 200 OK status code, example, sub-domains, JS files
							gospider -q -s "https://google.com/"
								flags:
									-q => quiet => only show urls
									-s "site"
									-S sites.txt
										Note:
											sites.txt should have http://site.com, not site.com
									-o output.txt 
							to get only subdomains:(https://github.com/harsh-kk/one_liner)
								gospider -d 0 -s "https://site.com" -c 5 -t 100 -d 5 --blacklist jpg,jpeg,gif,css,tif,tiff,png,ttf,woff,woff2,ico,pdf,svg,txt | grep -Eo '(http|https)://[^/"]+' | anew
					hakrawler:(github)
						installation:
							go get github.com/hakluke/hakrawler
						usage:
							hakrawler -url bugcrowd.com -depth 1
				Note:
					We will go with Burp Pro as:
						best way to do linked discovery is by using BURP PRO
						my methodology requires me to map the webapp manually and i will use burp via proxy too.
			JS analysis:
				looking in JS files for subdomains
				subdomainizer:(github)
					it is used to:
						find subdomains
						it also looks in JS files for sensitive info such as private API keys/creds.
					NOTE:
						its a valid vuln if we get private API keysor creds embedded in JS files. (sensitive data exposure)
					usage:
						-u url
						-l list_of_urls
						-o output.txt
				subscrapper:(github)
					it looks for ONLY subdomains.
				NOTE:
					wanna look in JS files for sensitive info + subdomains => subdomainizer
					wanna look in JS files only for subdomains => subscrapper
			Subdomain Scrapping:
				using google-fu/crt.sh/censys/etc to find subdomains
				Amass:
					amass enum -df domains.txt -o out.txt
				shosubgo:
					go script for Scrapping subdomains using SHODAN and it requires API key. we can also use AMASS for same by providing it with API key of shodan.
				scanning cloud ranges:(i think service is down)
					https://tls.bufferover.run
					the above site scans whole cloud ranges for AWS,GCP,AZURE using masscan for their SSL certs (port 443) and parse their SSL certs to domains name. This site do the scanning once every week, so it preety up-to date
					usage:
						curl 'https://tls.bufferover.run?q=site.com' 2>/dev/null
					it is also included in AMASS
			Subdomain bruteforcing:
				Note:
					all the subdomains found via 'Subdomain bruteforcing' are LIVE/ONLINE, so no need to use httprobe with them.
					we have a lot of tools to do subdomain bruteforcing, but issue with most tools is that they only use 1 'DNS RESOLVER' to do it, thus making the whole process very slow. So, we use AMASS
				Amass:
					by default, it uses 8 DNS RESOLVERS, but we can also specify our own resolvers.txt:
						amass enum -brute -df domains.txt -w wordlist.txt -rf resolvers.txt -o out.txt
						resolver.txt:
							https://raw.githubusercontent.com/janmasarik/resolvers/master/resolvers.txt
							the owner of above repo updates the resolvers.txt periodically
					wordlist.txt:
						all.txt:
							https://gist.githubusercontent.com/jhaddix/86a06c5dc309d08580a018c66354a056/raw/96f4e51d96b2203f19f6381c8c545b278eaa0837/all.txt
			Once, we are done with finding all the subdomains:
				merge all subdomains found except from those of "subdomain bruteforcing" into domains.txt
					reason: all the subdomains found via 'Subdomain bruteforcing' are LIVE/ONLINE, so no need to use httprobe with them.
				send that domains.txt to "httprobe":
					httprobe(github):(tomnomnom)
						used to find which subdomains in domains.txt are active
						installation:
							go get -u github.com/.../
						usage:	
							echo domains.txt | httprobe > final_domains.txt
				merge subdomains from subdomain bruteforcing into final_domains.txt
				cat final_domains.txt | uniq > final_domains.txt
				final_domains.txt should only have site.com not http://site.com or https://site.com
			Send final_domains.txt to "eyewitness":
				eyewitness(github):
					it looks for both http and https
					usage:
						./eyewitness.py -f final_domains.txt
						flags:
							-t timeout_in_seconds
								timeout means time taken by tool to render the site and take a screenshot
								-t 15 => toll will only take a screenshot if time to render is less than 15 seconds
							-d directory
								where to save all the screenshots
								by default: inside eyewitness directory
			Favicon Analysis:
				we make hash of favicon of site and search thi hash at shodan
				FavFreak:(github)
					usage:
						cat urls.txt | python3 favfreak.py -o output
						Note: 
							URLs must begin with either http or https
					benefits:
						1.it searches favicon hash on shodan
						2.it also has a db of common admin_page/login_framework such as wordpress favicon's hash and it uses this db to identify admin login pages or login_frameworks on given list of URLs
			what subdomains we are interested in:
				*testsite
				*dev
				*corp
				*stage
				*dasboard
			Note:
				www.site.com is correct
				ww2.site.com is ALSO CORRECT
		C. Github Dorking for Recon:(imp)(youtube.com/watch?v=l0YsEk_59fQ)
			Finding Sensitive Information Leaks:
				final_domainsto find sensitive files such as secret HTTP endpoints, session id, user info, passwords, API keys
			basics of github:
				repository => files for a project, can be public or private
					public => everyone can see
					private => only owner can see
					owner_name/repo_name
				commit => changes made to a code
			Manual:
				Login into github -> search bar
				company.com or company => show repo owned by a company (1000s of results)
				clearly, we cant see all 1000s of results, so:
					1. use keywords in search bar:
						"company.com" ssh
						"company.com" config
						keywords:
							https://github.com/random-robbie/keywords/blob/master/keywords.txt
							security_credentials => LDAP (AD)
							connectionstring => DB Creds
							jdbc => DB Creds(oracle)
							ssh2_auth_password => unauthorized access to servers
							send_keys => password
						Note:
							we need to utilize intelligence here, example, lets say a company uses apache server (we got to know via wappalyzer), then we only need to look for sensitive files which belong to an apache server, not an IIS server.
							So, always look for services used/products offered by a site.
						Note:
							if our scope is big, then using keywords such as password,etc will give many many results, so use words such as "jdbc"/"vsphere" instead
					2. many search results dont belong to company or its developers, so no use of checking them
					3. sort by: "recently indexed" => better chances of finding unreported data exposure. Look for files as old as 1 week/month
					4. NOT <string> filter:
						lets say, we have a subdomain out-of-scope and it has a lot of search results, so to save time:
							"company.com" ssh NOT subdomain.company.com
						NOT removes every result which includes <string> in it.
					6. org:company_name => looks for official repo of that company.
						NOTE:
							we should not look in official repo of company/registered emloyees as its a waste of time.
					7. NOT Looking in only "legitimate employee" code:
						Once, we find code from unregistered employee, visit his profile, look for him on linkedin, it will tell us if he is ex-employee or current employee.
						Once, we know he is a current employee:
							user:<username> password => to look if he sharerd his creds anywhere related to company or maybe he uses same password everywhere
				Note:
					if we find sensitive data related to internal server and we cant create a POC for that, submit only written report to company and its upto them, whether to accept it or not.
			Automated Tools:
				Why not to use them:
					tools go to the main page of an organization, search through their code/repo and then it goes to "people" tab (list of people who are "joined to that company on github"), Issue with this is, that, there are many developers of company who are not joined formally on github, so these tools dont search their profile, so most of the vulnerable scope is not even touched.
				gitrob:
					./gitrob --configure
					user:<my_github_username>
					password:<my_github_password>
					gitrob -o <company_name>
					go to http://127.0.0.1:9393/ => reports
		D. Wayback Machine for Recon:
			https://github.com/tomnomnom/waybackurls
		E. Shodan.io for Recon:
			Note:
				Shodan.io Recon is based on using filters to find "Hidden Assets" and to use FILTERS, we need to PAY.
			Common filters:(no use)
				os:"linux 2.6.x"
				product:"apache tomcat" version:"1.16.2"
				port:
			Useful Filters:
				org:"tesla motors"
				ssl:"target" => it looks in SSL for "target" string
				-"AkamaiGhost" => removes results having "AkamaiGhost" string in them
				html:"jenkins" => "html:" Searches full HTML content of the returned page and look for string(paid)
			
			Choosing a subdomain to take first:
				first target those subdomains whose user interface is of common monitoring tools/CMS
				in case of custom webapp, target those subdomains whose user interface deviates from the common company’s theme.
					we get to know how a site looks using eyewitness tool
Step-3 Testing the selected subdomain:
	A. SSL testing:(for weak ssl ciphers)
		nmap -p 443 --script=ssl-enum-ciphers site.com
	B. Port Scanning:
		it is done for 2 reasons:
			1. helps in finding RCE
			2. open ports => vuln
		nmap:(single subdomain)
			nmap -sC -sV <ip> -oA site_name
		scanning more than 1 subdomain:
			masscan:
				masscan -p1-65535 -iL ip_list --max-rate 18-- -oG out.log
				issues:
					it only takes in ip, not domain
			dnsmasscan:
				it takes domains.txt => coverts it to ip => feed those ip's to masscan
				dnsmasscan domains.txt dns.log -p1-65535 -oG masscan.log
					dns.log => log file having domains and ip's they resolve to
					masscan.log => results of masscan
		brutespray:
			it takes input as (-oG) file and it runs default password spray against open ssh/ftp/telnet/vnc/mssql/mysql/postgresql/rsh/imap/nntp/pcanywhere/pop3/rexec/rlogin/smbnt/smtp/svn/vmauthd/snmp
			python brutespray.py --file results.gnmap -U user.txt -P pass.txt --threads 5 --hosts 5
			flags:
				--service ssh,ftp
	C. Finding CVE's:
		Nuclei:
			Its configurable scanner based on templates that allows complete extensibility with a very simple templating syntax.
			usage:
				nuclei -l urls.txt -t template.yaml
			Templates:
				https://github.com/projectdiscovery/nuclei-templates
				its in YAML format
			Note:
				to use all templates:
					https://gist.github.com/dwisiswant0/23cc87d2149202652d265ab6ebe9e85f
					its a script nuclier.sh
	D. Mapping features of webapp:
		First, use webapp as normal unauthenciated user.
		Then, if there is a signup feature, ceate an account and login and visit every tab, click on every link, fill up every form.
		If it’s an e-commerce website, create an order using a fake credit card. 
		Look out for webapp features and make notes of interesting ones, example file uploads/data export/rich text editors/etc.
		Note:
			while doing it capture all the traffic with Burp.
		Note:
			It’s always tempting to switch between browser and Burp and ITS DISTRACTING.
			So, ONLY LOOK AT BROWSER DURING THIS STEP.	
	E. looking in comments and discovering "Disabled Functionality":
		Comments:
			open source-code -> search "<!--"
		Disabled Functionality:(reveals either previous or future section os site)
			is it really disabled or is there anywhere to invoke it?
	F. Directory Bruteforcing:
		Ffuf:
			USE FFUF NOT GOBUSTER AS FFUF IS FASTER.
			see its usage below.
		Note:
			there is no limit on dirbusting, we dont know which files are hidden.
			if ssh and http and linux box => try -x sh (shellshock)
			in case of IIS server, do -x aspx,asp
	G. Understanding architecture and defense mechanism of webapp:
		Go to Burp and look out for following questions:
			How authentication is made?
			Does the application use a third-party for that?
			Is there any OAuth flow?
			Is there any CSRF protection?
				If yes, how is it implemented?
			Are there any resources referenced using numerical identifiers?
				If yes, is there any protection against IDOR vulnerabilities?
			Does the application use any API?
			How does the application fetch data?
			Does it use a front-end Framework?
			What JavaScript files contain calls to the API?
			Does it use a back-end Framework?
				If yes, what is it and which version is being used?
	H. Focusing on one feature at a time:
		Goal is to learn the flow in detail, tinker with every user input based on my assumptions.
		Note:
			If you quit before this phase and jump to another asset or another totally different program, you will have lost all the time you have invested learning how the application works.
	I. Testing "authenciation mechanism":(login pages)
		Bruteforcing:
			We try to test authenciation mechanism by bruteforcing users. For that we need to know valid users on a site.
			Harvesting usernames:
				Test pairs of username:password and keep an eye on:
					Different HTML response
					Different Repspnse Variable:
						logon.php?reason=0 => wrong passwordd
						logon.php?reason=1 => wrong username
					Forms may repeat a valid username on refersh
		Authenciation flow bypas:
			make an account on site and visit all urls, then logout.
			visit every url which we visited with an authenciated account
				if we are able to see it => flaw.
	J. Testing Get parameters for available injection:
		sit.php?param=value
		always try these 3 values:
			1
			0
			-1
	K. Testing sessionid/cookie:
		base64/hash value
		if we remove the sessionid/cookie, do the site prompt us to login again? 
	L. In Case of CMS:(gitlab/webmin/etc)
		if we see configuration page => Misconfigured webapp 
		locate login and signup pages:
			On login page:
				try default creds
				bruteforcing using medusa/hydra/burp
				sqli
			On signup page:
				sqli
		once we become admin => upload shel
		Automated Tools:	
			Magneto:
				like WP, but for 'online stores'
				magescan:(github)
					php magescan.phar scan:all http://ip
			WebDAV:
				davtest --url http://<ip> => tells about type what file types can be uploaded/executed
				ASPX shell => msfvenom
			IIS 6.0:
				ScStoragePathfromURL BOF => github g0rx
			Joomla: 
				joomscan -u <url> => it tells versions of joomla
				--ec => enumerate components
				important pages:
					joomla/administrator => login page for admin
					joomla/configuration.php => configuration page
				    joomla/diagnostics.php
				    joomla/joomla.inc.php
				    joomla/config.inc.php
			Wordpress:
				NOTE: In HTB, if WP is present, always add domain_name in /etc/hosts file
				wpvulnhub.com => vulnerable plugins/themes
				important pages:
					wordpress/wp-login.php => login page for admin
					wordpress/wp-admin.php => admin page
					wordpress/wp-config.php => configuration page
					wordpress/wp-json/wp/v2/users => shows users registered on wordpress
				wpscan:
					usage:
						Vulnerability Scanner for WP
						Brutefocring accounts
						Enumerating users/plugins
					wpscan -u ip => to see vulnerabilities
					wpscan -u ip --disable-tls-check => https
					wpscan -u ip --enumerate u => enumerate users
						there may be some GUEST accounts open on WP
					wpscan -u ip --wordlist xx --username user –threads 2 => for bruteforcing
					wpscan --url <ip> -eu -ap -at
					--enumerate(e)  t => theme
									p => plugins
							   		ap => all plugins
							   		vp => vulnerable plugins
									u => usernames
					--wordlist => wordlist
					--username => username	
			Drupal:
				important pages:
					<ip>/drupal/settings.php => configuartion page (we can get db_pass and db_username here)
					ip/drupal/CHANGELOG.txt => it reflects the version and last update of drupal cms
				droopescan scan drupal -u <url>
					-u => url
					--enumerate p => plugins
								t => themes
								v => version of drupal installed
								a => all
				getting reverse shell in drupal:
					log in the cms
					goto Manage>Extend>List>Install new module
					download https://www.drupal.org/project/php and install it
					then enable it
					goto Manage > Extend >filters and enable the checkbox for PHP filters
			CMSmap:(WordPress|Joomla|Drupal|Moodle)
				Installation:
					git clone https://github.com/Dionach/CMSmap
					edit cmsmap.conf:
						[exploitdb]
						edbtype = APT
						edbpath = /usr/share/exploitdb/
					pip3 install .
				Usage:	
		Note:
			always google "webapp_name source version_number" to see source files which may have left behind on webserver after installation
	M. Exploiting WebApp:
		if (age<48 hours)
			look for low hanging
	N. Other Important Stuff:
		Avoid Duplicates in Bug Bounty:
			Duplicates are also known as dupes
			When to look for low hanging fruit:
				For only first 48 hours after program went out public
				XSS in an input on the Contact Us page of a web app
				Reflected XSS in 'Search' input
				Open Redirect in URL
				hardcoded credentials in HTML or JavaScript source
				SQL Injection on ID parameter in URL
				LFI in something like index.php?include=about/careers.ph
			Avoid low hanging fruit:
				IT IS NOT A PART OF OWASP TOP 10
				If a low hanging bug is reported and if it’s hard to resolve, company will ignore it and it will be still there and the next hacker hacking on there will find it and report them and end up with duplicates and demotivation.
				SO ONLY FOCUS ON OWASP TOP 10 and find them
				SSRF IS NOT A LOW HANGING FRUIT
				CSRF IS NOT A LOW HANGING FRUIT
				SSTI IS NOT A LOW HANGING FRUIT
				SQLi IS NOT A LOW HANGING FRUIT
			What bugs to look for then?:
				XSS in a really obscure part of the applciation, or on another in-scope system which is hard to find
				The more obscure the location the better
				The goal here is to look in places the other hunters probably haven’t looked yet. Bugs like these often come as result of HTML source review for clues which point to unlinked pages.
				So, dont look at make profile, edit profile, contact us, comment, search bar
			Avoid reporting bugs from automated tools:
				Burp Suite
				Almost every hunter uses Burp and other common automated tools, it means that he also has found same issues, and probably reported them earlier.
		Avoid out of scope issues:
			CSRF on logout
			self XSS with no security impact
			reports, which are defined as 'Out of scope' or 'Duplicate', those reports likely will end up as 'Not applicable' or 'N/A' , which has consequences including loosing reputation points or even disqualification from the program.
		Hacktivity:
			allows to see all resolved reports
			sort them by popularity or age
			filter them by company or bug_name
			search through them using keywords
			It helps to answer questions such as:
				what types of bugs are the most popular?
				Which services are most attacked?
				How many bugs were resolved so far?
				How many researchers were revarded?
				When the last bugs were disclosed?
		Low Hanging Fruits:
			IT IS NOT A PART OF OWASP TOP 10
			If a low hanging bug is reported and if it’s hard to resolve, company will ignore it and it will be still there and the next hacker hacking on there will find it and report them and end up with duplicates and demotivation.
			Examples:	
				XSS in an input on the Contact Us page of a web app
				Reflected XSS in 'Search' input
				Open Redirect in URL
				hardcoded credentials in HTML or JavaScript source
				SQL Injection on ID parameter in URL
				LFI in something like index.php?include=about/careers.ph
Amass:(github)
	Installation:(kali linux)
		apt-get update
		apt-get install amass
	Usage:
		It has 5 SUBCOMMANDS(modes):
			intel => Discover targets for enumerations
			enum => Perform enumerations and network mapping
			viz	=> Visualize enumeration results
			track => Track differences between enumerations
			db => Manipulate the Amass graph database
		config file:($HOME/.config/amass/config.ini)
			stores all API keys
		normal flags:
			-d domain.com
			-df domains.txt
				note:
					domains.txt should have site.com, not https://site.com 
			-asn asn_number
			-o output.txt
			-p port,port
			-ip => show ip of assets found
			-config config.ini => include config file
			-active => go active
				amass sends traffic to assets to get TLS cert
			-pasive => go passive
			NOTE:
				sometimes, we dont use either -passive or -active, then, it uses DNS request traffic to send to resolvers
		intel:
			amass intel -whois -d domain.com
			-whois => reverse whois
		enum:
			-brute => subdomain bruteforcing
			-w wordlist.txt
			-nf names.txt => names.txt includes those subdoamin which we already have found from using other tools
		commands to use:
			amass intel -whois -d domain.com -asn <number>
			amass enum -d domain.com -asn <number>
			amass enum -brute -w word.txt -d domain.com -asn <number>
	API keys:
		Using Amass without setting up API keys is like eating sand. It tastes bland and is bad for your health.
		AP keys	are very important
		Free Keys:
			AlienVault
			BinaryEdge
			BuiltWith
			Censys
			Chaos
			GitHub
			NetworksDB
			PassiveTotal
			RapidDNS
			Shodan
			VirusTotal
			WhoisXML
Ffuf:(github)
	fast web fuzzer written in go. it can fuzz anything
	used in dirbusting
	installation:
		sudo apt-get install ffuf
	Usage:
		ffuf -u https://site.com/FUZZ -w wordlist.txt
		ffuf -u https://site.com/FUZZ -w wordlist -recursion -e .aspx,.html => find files with a specific extension	
			flags:
				-recursion => recursive search
				-recursion-depth => to specify depth of recursion
				-e .ext1,.ext2 => to specify extensions
		Fuzzing Multiple Locations:
			ffuf -u https://W2/W1 -w ./wordlist.txt:W1,./domains.txt:W2 => basic usage
			ffuf -u https://FUZZDOMAIN/FUZZDIR -w ./wordlist.txt:FUZZDIR,./domains.txt:FUZZDOMAIN 
				ffuf will try every directory for the first domain, then every directory on the second domain. When running with many threads, this means sending 1000 requests to the same server in a very short amount of time. This often leads to getting rate-limited or banned.
			ffuf -u https://FUZZDOMAIN/FUZZDIR -w ./domains.txt:FUZZDOMAIN,./wordlist.txt:FUZZDIR 
				ffuf will try the first directory on all domains, before moving on to the next directory and trying that on all domains. This way you can send more requests without overloading the target servers.
		Bruetforcing login pages:
			Bruteforcing either username or password:
				ffuf -u https://site.com/login.php?user=la&pass=FUZZ -w wordlist.txt => get param
				ffuf -u https://site.com -X POST -d “username=admin\&password=FUZZ” -w wordlist.txt => post param
					flags:	
						-X <method> => to specify http method
						-d "request data" => to specify the data
			Bruteforcing both username and password:
				ffuf -request req.txt -request-proto http -mode clusterbomb -w usernames.txt:UFUZZ,passwords.txt:PFUZZ
					flags:
						-request req.text => specify a file with raw HTTP Request
							copy request from burp and paste it in req.text
							In the request file, UFUZZ is placed at login_username and PFUZZ is placed at login_password
						-request-proto =>
						-mode <attack_type> => to specify attack type:
							clusterbomb:
								Every word in user wordlist will be used with every word in pass wordlist
							pitchfork:
								Word at first position in user wordlist will be used with word at first position in pass wordlist.
								If the number of words in both lists are not same then the attack will stop as soon as the list with lesser number of words gets exhausted.
		other flags:
			-b "cookie" => to specify cookie data for authenciated fuzzing
			-t 50 => to specify no of threads (default => 40)
			-s => silent mode
			-c => coloured output
			-v => verbose mode
			-maxtime 60 => to end fuzzing after 60 seconds
			-timeout 5 => To set a timeout for each request in seconds (default => 10)
			Output:
			-o out.txt => ouput
			-of html => to write output in other format
				ffuf -u https://site.com/FUZZ -w wordlist.txt -of html -o ./output
				allowed formats:
					json
					ejson
					html
					md
					csv
					ecsv
			Filtering:
				To show:(To Match)
					-mc 200,302 => to specify Status code
					-ml => to specify amount of lines in response
					-mr => to specify regex pattern
					-mw => to specify amount of words in response
					-ms => to specify response size
				To not show:(To Filter out)
					-fc 200,302 => to filter Status code
					-fl => to filter amount of lines in response
					-fr => to filter regex pattern
					-fw => to filter amount of words in response
					-fs => to filter response size
		Note:
			wordlist for content discovery:(dirbusting)
				seclists/Discovery/Web-Content/directory-list-2.3-medium.txt
ssl cert:(CTF)
	we need to see ssl cert, as it may tell us emails/names/subdomains/domains
/etc/hosts:(CTF)
	sometimes, we need to put ip and domain_name to see actual site
		IP domain1 domain2
Using VPS:(Virtual Private Server)
	Basics:
		VPS is a shared server
		benfits:
			More bandwidth
			Keep it running 24/7!
			conduct scans from a separate IP without switching VPN bcoz of IP bans.
			host various services vital to testing, example: if testing for XSS or SSRF vulnerabilities it is nice to have a VPS that the target server can call back to.
			receive reverse shell callbacks or host web content for proof-of-concepts.
		I NEED LINUX SERVER
		choices:
			https://cloud.google.com/free
				https://medium.com/bugbountywriteup/beginners-guide-vps-setup-for-bug-bounty-recon-automation-6b0ba1e051ef
			https://www.genesishosting.com/genesis_vms_pricing_table.html
		To install tools:
			https://github.com/pizza-power/bug-bounty-box-setup/blob/master/setup.sh
	Free VPS:(Using Google Collab)
		Go-To https://colab.research.google.com/github/hackingguy/Bug-Hunting-Colab/blob/master/Bug_Hunting_Colab.ipynb:
			Edit -> NoteBook Settings -> Hardware Accelerator: GPU
			Runtime -> Factory reset runtime
			Click Here to Install Tools And Create SSH Tunnel:
				enable "CREATE_VNC" 
				run
				Note: ssh wont work like that
			clicl on Reconftw:(this will install all tools)
				run
			Click on Wetty:(this provides terminal over http or https)
				enable "USE_FREE_TOKEN"
				run
			click on mount GDrive:
				this will mount drive at /content/drive
		Benefits:
			If You Use GPU Instance ,You will get 32 GB RAM and 70 GB Storage with combination of Nvidia Tesla K8 GPU (Hashcat will be Extremely Fast).
			A Normal Instance will provide 16 GB RAM and 100 GB of Storage
